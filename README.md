In this project, I look at the following several academic papers that overlook the topics of explainability and interpretability:
<ol>
  <li>Sensible AI: Re-imagining Interpretability and Explainability Using Sensemaking Theory</li>
  <li>Explainability as a User Requirement for Artificial Intelligence Systems</li>
  <li>Scope and Sense of Explainability for AI-Systems</li>
  <li>How to Quantify the Degree of Explainability: Experiments and Practical Implications</li>
  <li>The Road to Explainability is Paved with Bias: Measuring the Fairness of Explanations</li>
  <li>Explainability and Performance of Anticipatory Learning Classifier Systems in Non-Deterministic Environments</li>
  <li>Explainability’s Gain is Optimality’s Loss?: How Explanations Bias Decision-making</li>
  <li>Materializing Interpretability: Exploring Meaning in Algorithmic Systems</li>
  <li>Balancing the Tradeoff Between Clustering Value and Interpretability</li>
  <li>Interpretable Machine Learning in Healthcare</li>
</ol>
then I do my own personal sentiment analysis on them, next, I do sentiment analysis using the Flair NLP tool, the documentation for which can be found here: https://github.com/flairNLP/flair. Finally, I will compare and contrast my results with that of Flair and will compile my findings into a single paper.  

The goal of this project is to analyze and contextualize the way that the topics of explainability and interpretability are discussed in order to understand their futures.

In my 
~~~
code
~~~
directory, you will be able to find the actual line-by-line results of my Flair analysis, and the code I use to get a singular sentiment score for each paper. 
